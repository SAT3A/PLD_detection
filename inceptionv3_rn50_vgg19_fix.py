# -*- coding: utf-8 -*-
"""INCEPTIONV3_RN50_VGG19_FIX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CqlbNOM9PbZpvuUMs9Y9l-b62OlWSLAp

##**TRANSFER LEARNING**
"""



"""##**IMPORT LIBRARY YANG DIBUTUHKAN**


"""

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
import zipfile
import matplotlib.image as mpimg
import time
import numpy as np
import keras


from tensorflow import keras
from keras.preprocessing import image
from matplotlib import pyplot as plt
from google.colab import files
from keras.preprocessing import image
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications import VGG19
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.applications.vgg19 import preprocess_input
from tensorflow.keras import layers
from keras.utils.vis_utils import plot_model

"""##**DATASET DAN DATALOADER**"""

local_zip = '/content/drive/MyDrive/DATASET/DATASET_PLD_V1.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')

zip_ref.close()

#DATA TRAINING
training_healthy = os.path.join('/content/DATASET_PLD_V1/Training/Healthy')
training_early_blight  = os.path.join('/content/DATASET_PLD_V1/Training/Early_Blight')
training_late_blight = os.path.join('/content/DATASET_PLD_V1/Training/Late_Blight')

#DATA VALIDATION
validation_healthy = os.path.join('/content/DATASET_PLD_V1/Validation/Healthy')
validation_early_blight = os.path.join('/content/DATASET_PLD_V1/Validation/Early_Blight')
validation_late_blight = os.path.join('/content/DATASET_PLD_V1/Validation/Late_Blight')

training_path = '/content/DATASET_PLD_V1/Training'
validation_path = '/content/DATASET_PLD_V1/Validation'
test_path = '/content/DATASET_PLD_V1/Testing'

"""##**EXPLORASI DATA**"""

#=============================================================#

print('\n DATA TRAINING \n')
training_healthy_names = os.listdir(training_healthy)
print(training_healthy_names[:10])

training_early_blight_names = os.listdir(training_early_blight)
print(training_early_blight_names[:10])

training_late_blight_names = os.listdir(training_late_blight)
print(training_late_blight_names[:10])

#=============================================================#

print('\n DATA VALIDATION \n')
validation_healthy_names = os.listdir(validation_healthy)
print(validation_healthy_names[:10])

validation_early_blight_names = os.listdir(validation_early_blight)
print(validation_early_blight_names[:10])

validation_late_blight_names = os.listdir(validation_late_blight)
print(validation_late_blight_names[:10])

print('total training PLD Healthy images:', len(os.listdir(training_healthy)))
print('total training PLD Early Blight images:', len(os.listdir(training_early_blight)))
print('total training PLD Late Blight images:', len(os.listdir(training_late_blight)),'\n')

print('total validation PLD Healthy images:', len(os.listdir(validation_healthy)))
print('total validation PLD Early Blight images:', len(os.listdir(validation_early_blight)))
print('total validation PLD Late Blight images:', len(os.listdir(validation_late_blight)),'\n')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# Parameters for our graph; we'll output images in a 5x5 configuration
nrows = 5
ncols = 5

# Index for iterating over images
pic_index = 0

fig = plt.gcf()
fig.set_size_inches(ncols * 5, nrows * 5)

pic_index += 10

train_hl_pic = [os.path.join(training_healthy, fname)
                for fname in training_healthy_names[pic_index-5:pic_index]]
train_eb_pic = [os.path.join(training_early_blight, fname)
                for fname in training_early_blight_names[pic_index-5:pic_index]]
train_lb_pic = [os.path.join(training_late_blight, fname)
                for fname in training_late_blight_names[pic_index-5:pic_index]]

for i, img_path in enumerate(train_hl_pic + train_eb_pic + train_lb_pic):
  # Set up subplot; subplot indices start at 1
  sp = plt.subplot(nrows, ncols, i + 1)
  

  img = mpimg.imread(img_path)
  plt.axis('off')
  plt.imshow(img)

plt.show()

"""##**DATA AUGMENTASI**"""

#DATA AUGMENTASI UNTUK INCEPTION V3 dan VGG19
train_datagenDA = ImageDataGenerator(rescale = 1/255.,rotation_range = 40, 
                                  brightness_range = [1,2],
                                  zoom_range = 0.1,
                                   horizontal_flip = True)

valid_datagenDA = ImageDataGenerator(rescale = 1/255)
test_datagenDA = ImageDataGenerator(rescale = 1/255)


#DATA AUGMENTASI UNTUK RESNET50
train_datagen_pi = ImageDataGenerator(preprocessing_function=preprocess_input,rotation_range = 40, 
                                  brightness_range = [1,2],
                                  zoom_range = 0.1,
                                   horizontal_flip = True )

valid_datagen_pi = ImageDataGenerator(preprocessing_function=preprocess_input)
test_datagen_pi = ImageDataGenerator(preprocessing_function=preprocess_input)

image_sample_path = '/content/DATASET_PLD_V1/Testing/Healthy/Healthy_2.jpg'

#Loads image in from the set image path
img = keras.preprocessing.image.load_img(image_sample_path, target_size= (100,100))
img_tensor = keras.preprocessing.image.img_to_array(img)
img_tensor = np.expand_dims(img_tensor, axis=0)
#Allows us to properly visualize our image by rescaling values in array
img_tensor /= 255.
#Plots image
plt.figure(figsize=(8,4))
plt.imshow(img_tensor[0])
plt.show()

#Loads in image path
img = keras.preprocessing.image.load_img(image_sample_path, target_size= (200,200))
img_tensor = keras.preprocessing.image.img_to_array(img)
img_tensor = np.expand_dims(img_tensor, axis=0)
#Uses ImageDataGenerator to flip the images
datagen = ImageDataGenerator(rotation_range = 45, 
                                  #width_shift_range = 0.2,
                                  #height_shift_range = 0.2,
                                  brightness_range = [1,2],
                                  zoom_range = 0.1,
                                  horizontal_flip = True,
                                  vertical_flip = True)
#Creates our batch of one image
pic = datagen.flow(img_tensor, batch_size =1)
plt.figure(figsize=(50,25))
#Plots our figures
for i in range(1,6):
  plt.subplot(1, 5, i)
  batch = pic.next()
  image_ = batch[0].astype('uint8')
  plt.imshow(image_)
plt.show()

"""##**PREPROCESSING DATA**"""

batch_size = 32

#Mengatur target size gambar berukuran sebesar 200 x 200
train_generatorDA = train_datagenDA.flow_from_directory(training_path,
                                                    classes = ['Healthy', 'Early_Blight', 'Late_Blight'],
                                                    class_mode = 'categorical',                                              
                                                    color_mode = "rgb",
                                                    batch_size = batch_size,
                                                    target_size = (200, 200),
                                                    shuffle = True)


validation_generatorDA = valid_datagenDA.flow_from_directory(validation_path,
                                                    classes = ['Healthy', 'Early_Blight', 'Late_Blight'],
                                                    class_mode = 'categorical',
                                                    color_mode = "rgb",
                                                    batch_size = batch_size,
                                                    target_size = (200, 200),
                                                    shuffle = False)

train_generatorRN = train_datagen_pi.flow_from_directory(training_path,
                                                    classes = ['Healthy', 'Early_Blight', 'Late_Blight'],
                                                    class_mode = 'categorical',                                              
                                                    color_mode = "rgb",
                                                    batch_size = batch_size,
                                                    target_size = (200, 200),
                                                    shuffle = True)

validation_generatorRN = valid_datagen_pi.flow_from_directory(validation_path,
                                                    classes = ['Healthy', 'Early_Blight', 'Late_Blight'],
                                                    class_mode = 'categorical',
                                                    color_mode = "rgb",
                                                    batch_size = batch_size,
                                                    target_size = (200, 200),
                                                    shuffle = False)

class_names = train_generatorDA.class_indices
class_names

len(class_names)

"""##**================== INCEPTION V3 =====================**

##**INCEPTIONV3**
"""

from tensorflow.keras.applications import InceptionV3
import tensorflow as tf

base_model_INCV3_V15 = InceptionV3(input_shape= (200, 200, 3),
                                include_top = False,
                                pooling = 'max',
                                weights = 'imagenet')

for layer in base_model_INCV3_V15.layers:
  layer.trainable = False

base_model_INCV3_V15.summary()

#Flatenning 
x = layers.Flatten()(base_model_INCV3_V15.output)

#Fully Connected Layer
x = layers.Dense(512, activation='relu')(x)

#Dropout Layer
x = layers.Dropout(0.2)(x)

#menambahkan layar output dengan fungsi softmax untuk klasifikasi 
x = layers.Dense (3, activation='softmax')(x)


MODEL_INCEPTIONV3_V15 = tf.keras.models.Model(base_model_INCV3_V15.input, x)
MODEL_INCEPTIONV3_V15.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

MODEL_INCEPTIONV3_V15.summary()

start_time = time.time()

hist_incv3DA = MODEL_INCEPTIONV3_V15.fit(train_generatorDA, validation_data = validation_generatorDA,
                 steps_per_epoch = len(train_generatorDA), epochs = 10,
                 validation_steps = len(validation_generatorDA))

print("Running time : --- %s secods ---" % (time.time() - start_time))

acc_loss_incV3_DA = MODEL_INCEPTIONV3_V15.evaluate(validation_generatorDA)

print('Accuracy           : ', acc_loss_incV3_DA[1]*100)
print('Loss               : ', acc_loss_incV3_DA[0])

acc = hist_incv3DA.history['accuracy']
val_acc = hist_incv3DA.history['val_accuracy']

loss = hist_incv3DA.history['loss']
val_loss = hist_incv3DA.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""##**FINE TUNING INCEPTIONV3**"""

base_model_INCV3_V15.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model_INCV3_V15.layers))

MODEL_INCEPTIONV3_V15.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-5),
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

MODEL_INCEPTIONV3_V15.summary()

start_time = time.time()

fine_tune_epochs = 10
initial_epochs = 10
total_epochs =  initial_epochs + fine_tune_epochs

history_fineDA = MODEL_INCEPTIONV3_V15.fit(train_generatorDA,
                         validation_data=validation_generatorDA,
                         epochs=total_epochs,
                         steps_per_epoch = len(train_generatorDA),
                         initial_epoch=hist_incv3DA.epoch[-1],
                         validation_steps = len(validation_generatorDA)
                         )
print("Running time : --- %s secods ---" % (time.time() - start_time))

acc += history_fineDA.history['accuracy']
val_acc += history_fineDA.history['val_accuracy']

loss += history_fineDA.history['loss']
val_loss += history_fineDA.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

acc_loss_finetuneDA = MODEL_INCEPTIONV3_V15.evaluate(validation_generatorDA)
print('accuracy = ', acc_loss_finetuneDA[1]*100)
print('loss     = ', acc_loss_finetuneDA[0])

"""##**PERBANDINGAN FASE 1 DENGAN FASE 2**"""

acc_loss_incV3_DA

print('Akurasi Pada Fase 1 : ', acc_loss_incV3_DA[1]*100)
print('Loss Pada Fase 1    : ', acc_loss_incV3_DA[0])
print('==================================================')
print('Akurasi Pada Fase 2 : ', acc_loss_finetuneDA[1]*100)
print('Loss Pada Fase 2    : ', acc_loss_finetuneDA[0])
print('==================================================')


X = ['Adaptasi','Setelah Fine Tuning']
YAccuracy = [acc_loss_incV3_DA[1]*100,acc_loss_finetuneDA[1]*100,]
ZLoss = [acc_loss_incV3_DA[0]*100,acc_loss_finetuneDA[0]*100,]
  
X_axis = np.arange(len(X))
  
plt.bar(X_axis - 0.1, YAccuracy, 0.2, label = 'Accuracy', )
plt.bar(X_axis + 0.1, ZLoss, 0.2, label = 'Loss', )
  
plt.xticks(X_axis, X)
plt.ylabel("")
plt.ylabel("")
plt.title("Perbandingan Akurasi dan Loss Fase 1 dengan Fase 2")
plt.legend(loc='upper right')
plt.show()

"""##**UJI DENGAN DATA TEST**"""

test_dataset = tf.keras.utils.image_dataset_from_directory(test_path,
                                                                 shuffle=True,
                                                                 batch_size=32,
                                                                 image_size=(200,200))

test_datasetDA = test_datagenDA.flow_from_directory(test_path,
                                                    classes = ['Healthy', 'Early_Blight', 'Late_Blight'],
                                                    class_mode = 'categorical',
                                                    color_mode = "rgb",
                                                    batch_size = 32,
                                                    target_size = (200, 200),
                                                    shuffle = False)

test_class_names = test_dataset.class_names
test_class_names

import seaborn as sns
from sklearn.metrics import precision_score, recall_score,\
                            confusion_matrix, classification_report, \
                            accuracy_score, f1_score, plot_confusion_matrix

pred_INCV3_DA = np.argmax(MODEL_INCEPTIONV3_V15.predict(test_datasetDA), axis = -1)

def INCV3_report(predictions):
    ax = plt.subplot()
    sns.heatmap(confusion_matrix(test_datasetDA.classes,pred_INCV3_DA), annot = True, cmap="Greens", fmt='g', ax=ax)
    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
    ax.set_title('Confusion Matrix INCEPTION V3');
    ax.xaxis.set_ticklabels(['Healthy', 'Early Blight', 'Late Blight']); ax.yaxis.set_ticklabels(['Healthy', 'Early Blight', 'Late Blight']);

cm1_DA = confusion_matrix(test_datasetDA.classes, pred_INCV3_DA)
INCV3_report(pred_INCV3_DA)
benar_V1 = cm1_DA[0][0] + cm1_DA[1][1] + cm1_DA[2][2]
salah_V1 = cm1_DA[0][1] + cm1_DA[0][2] + cm1_DA[1][0] + cm1_DA[1][2] + cm1_DA[2][0] + cm1_DA[2][1] 
print('Jumlah prediksi benar dari 306 gambar = ', benar_V1)
print('Jumlah prediksi salah dari 306 gambar = ', salah_V1,'\n')

cm1_DA = confusion_matrix(test_datasetDA.classes, pred_INCV3_DA)

print('===================CONFUSION MATRIX INCEPTIONV3===================== \n')
print('[TP] True positive   = ', cm1_DA[0][0])
print('[FP] False positive  = ', cm1_DA[1][0] + cm1_DA[2][0])
print('[FN] False negative  = ', cm1_DA[0][1] + cm1_DA[0][2])
print('[TN] True negative   = ', cm1_DA[1][1] + cm1_DA[1][2] + cm1_DA[2][1] + cm1_DA[2][2])

TP = cm1_DA[0][0]
FP = cm1_DA[1][0] + cm1_DA[2][0]
FN = cm1_DA[0][1] + cm1_DA[0][2]
TN = cm1_DA[1][1] + cm1_DA[1][2] + cm1_DA[2][1] + cm1_DA[2][2]


Sensitivity_INCV3_DA = TP / ( TP + FN )
Specificity_INCV3_DA = TN / ( TN + FP )

print("\nSensitivity INCV3    = ", Sensitivity_INCV3_DA*100)
print("Spesificity INCV3    = ", Specificity_INCV3_DA*100)

for images_batch, labels_batch in test_dataset.take(1):
  first_image = images_batch[1].numpy().astype('uint8')
  first_label = labels_batch[1]
  plt.imshow(first_image)
  plt.axis('off')
  batch_prediction = MODEL_INCEPTIONV3_V15.predict(images_batch)
  print('Label Images      : ', test_class_names[first_label])
  print('Prediction Result : ', test_class_names[np.argmax(batch_prediction[1])],'\n')

"""##**================== RESNET50 =====================**

##**RESNET50**
"""

from tensorflow.keras.applications import ResNet50
import tensorflow as tf

base_model_RN50_V15 = ResNet50(input_shape= (200, 200, 3),
                                include_top = False,
                                pooling='max',
                                weights = 'imagenet')

for layer in base_model_RN50_V15.layers:
  layer.trainable = False

base_model_RN50_V15.summary()

#Flatenning 
x = layers.Flatten()(base_model_RN50_V15.output)

#Fully Connected Layer
x = layers.Dense(512, activation='relu')(x)

#Dropout Layer
x = layers.Dropout(0.2)(x)

#menambahkan layar output dengan fungsi softmax untuk klasifikasi 
x = layers.Dense (3, activation='softmax')(x)


MODEL_RESNET50_V15= tf.keras.models.Model(base_model_RN50_V15.input, x)
MODEL_RESNET50_V15.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

MODEL_RESNET50_V15.summary()

start_time = time.time()

hist_RN50 = MODEL_RESNET50_V15.fit(train_generatorRN, validation_data = validation_generatorRN,
                 steps_per_epoch = len(train_generatorRN), epochs = 10,
                 validation_steps = len(validation_generatorRN))

print("Running time : --- %s secods ---" % (time.time() - start_time))

acc_loss_rn50 = MODEL_RESNET50_V15.evaluate(validation_generatorRN)

print('Accuracy           : ', acc_loss_rn50[1]*100)
print('Loss               : ', acc_loss_rn50[0])

acc = hist_RN50.history['accuracy']
val_acc = hist_RN50.history['val_accuracy']

loss = hist_RN50.history['loss']
val_loss = hist_RN50.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""##**FINE TUNING RESNET50**"""

base_model_RN50_V15.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model_RN50_V15.layers))

MODEL_RESNET50_V15.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-5),
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

MODEL_RESNET50_V15.summary()

start_time = time.time()

fine_tune_epochs = 10
initial_epochs = 10
total_epochs =  initial_epochs + fine_tune_epochs

history_rn50_fine = MODEL_RESNET50_V15.fit(train_generatorRN,
                         validation_data=validation_generatorRN,
                         epochs=total_epochs,
                         steps_per_epoch = len(train_generatorRN),
                         initial_epoch=hist_RN50.epoch[-1],
                         validation_steps = len(validation_generatorRN)
                         )
print("Running time : --- %s secods ---" % (time.time() - start_time))

acc += history_rn50_fine.history['accuracy']
val_acc += history_rn50_fine.history['val_accuracy']

loss += history_rn50_fine.history['loss']
val_loss += history_rn50_fine.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

acc_loss_rn50_finetuneDA = MODEL_RESNET50_V15.evaluate(validation_generatorRN)
print('accuracy = ', acc_loss_rn50_finetuneDA[1]*100)
print('loss     = ', acc_loss_rn50_finetuneDA[0])

"""##**PERBANDINGAN FASE 1 DENGAN FASE 2**"""

acc_loss_rn50

print('Akurasi Pada Fase 1 : ', acc_loss_rn50[1]*100)
print('Loss Pada Fase 1    : ', acc_loss_rn50[0])
print('==================================================')
print('Akurasi Pada Fase 2 : ', acc_loss_rn50_finetuneDA[1]*100)
print('Loss Pada Fase 2    : ', acc_loss_rn50_finetuneDA[0])
print('==================================================')


X = ['Adaptasi','Setelah Fine Tuning']
YAccuracy = [acc_loss_rn50[1]*100,acc_loss_rn50_finetuneDA[1]*100,]
ZLoss = [acc_loss_rn50[0]*100,acc_loss_rn50_finetuneDA[0]*100,]
  
X_axis = np.arange(len(X))
  
plt.bar(X_axis - 0.1, YAccuracy, 0.2, label = 'Accuracy', )
plt.bar(X_axis + 0.1, ZLoss, 0.2, label = 'Loss', )
  
plt.xticks(X_axis, X)
plt.ylabel("")
plt.ylabel("")
plt.title("Perbandingan Akurasi dan Loss Fase 1 dengan Fase 2")
plt.legend(loc='upper right')
plt.show()

"""##**UJI DENGAN DATA TEST**"""

test_dataset = tf.keras.utils.image_dataset_from_directory(test_path,
                                                                 shuffle=False,
                                                                 batch_size=32,
                                                                 image_size=(200,200))

test_datasetRN = test_datagen_pi.flow_from_directory(test_path,
                                                    classes = ['Healthy', 'Early_Blight', 'Late_Blight'],
                                                    class_mode = 'categorical',
                                                    color_mode = "rgb",
                                                    batch_size = 32,
                                                    target_size = (200, 200),
                                                    shuffle = False)

test_class_names = test_dataset.class_names
test_class_names

import seaborn as sns
from sklearn.metrics import precision_score, recall_score,\
                            confusion_matrix, classification_report, \
                            accuracy_score, f1_score, plot_confusion_matrix

pred_RN50 = np.argmax(MODEL_RESNET50_V15.predict(test_datasetRN), axis = -1)

def RESNET50_report(predictions):
    ax = plt.subplot()
    sns.heatmap(confusion_matrix(test_datasetRN.classes,pred_RN50), annot = True, cmap="Greens", fmt='g', ax=ax)
    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
    ax.set_title('Confusion Matrix RESNET50');
    ax.xaxis.set_ticklabels(['Healthy', 'Early Blight', 'Late Blight']); ax.yaxis.set_ticklabels(['Healthy', 'Early Blight', 'Late Blight']);

cm2_DA = confusion_matrix(test_datasetRN.classes, pred_RN50)
RESNET50_report(pred_RN50)
benar_V2 = cm2_DA[0][0] + cm2_DA[1][1] + cm2_DA[2][2]
salah_V2 = cm2_DA[0][1] + cm2_DA[0][2] + cm2_DA[1][0] + cm2_DA[1][2] + cm2_DA[2][0] + cm2_DA[2][1] 
print('Jumlah prediksi benar dari 306 gambar = ', benar_V2)
print('Jumlah prediksi salah dari 306 gambar = ', salah_V2,'\n')

cm2_DA = confusion_matrix(test_datasetRN.classes, pred_RN50)

print('===================CONFUSION MATRIX RESNET50===================== \n')
print('[TP] True positive   = ', cm2_DA[0][0])
print('[FP] False positive  = ', cm2_DA[1][0] + cm2_DA[2][0])
print('[FN] False negative  = ', cm2_DA[0][1] + cm2_DA[0][2])
print('[TN] True negative   = ', cm2_DA[1][1] + cm2_DA[1][2] + cm2_DA[2][1] + cm2_DA[2][2])

TP = cm2_DA[0][0]
FP = cm2_DA[1][0] + cm2_DA[2][0]
FN = cm2_DA[0][1] + cm2_DA[0][2]
TN = cm2_DA[1][1] + cm2_DA[1][2] + cm2_DA[2][1] + cm2_DA[2][2]


Sensitivity_RN50_DA = TP / ( TP + FN )
Specificity_RN50_DA = TN / ( TN + FP )

print("\nSensitivity INCV3    = ", Sensitivity_RN50_DA*100)
print("Spesificity INCV3    = ", Specificity_RN50_DA*100)

for images_batch, labels_batch in test_dataset.take(1):
  first_image = images_batch[0].numpy().astype('uint8')
  first_label = labels_batch[0]
  plt.imshow(first_image)
  plt.axis('off')
  batch_prediction = MODEL_RESNET50_V15.predict(images_batch)
  print('Label Images      : ', test_class_names[first_label])
  print('Prediction Result : ', test_class_names[np.argmax(batch_prediction[0])],'\n')

"""##**================== VGG19 =====================**

##**VGG19**
"""

from tensorflow.keras.applications import VGG19
import tensorflow as tf

base_model_VGG19_V15 = VGG19(input_shape= (200, 200, 3),
                                include_top = False,
                                pooling = 'max',
                                weights = 'imagenet')

for layer in base_model_VGG19_V15.layers:
  layer.trainable = False

base_model_VGG19_V15.summary()

#Flatenning 
x = layers.Flatten()(base_model_VGG19_V15.output)

#Fully Connected Layer
x = layers.Dense(512, activation='relu')(x)

#Dropout Layer
x = layers.Dropout(0.2)(x)

#menambahkan layar output dengan fungsi softmax untuk klasifikasi 
x = layers.Dense (3, activation='softmax')(x)


MODEL_VGG19_V15 = tf.keras.models.Model(base_model_VGG19_V15.input, x)
MODEL_VGG19_V15.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

MODEL_VGG19_V15.summary()

start_time = time.time()

hist_VGG19 = MODEL_VGG19_V15.fit(train_generatorDA, validation_data = validation_generatorDA,
                 steps_per_epoch = len(train_generatorDA), epochs = 10,
                 validation_steps = len(validation_generatorDA))

print("Running time : --- %s secods ---" % (time.time() - start_time))

acc_loss_vgg19 = MODEL_VGG19_V15.evaluate(validation_generatorDA)

print('Accuracy           : ', acc_loss_vgg19[1]*100)
print('Loss               : ', acc_loss_vgg19[0])

acc = hist_VGG19.history['accuracy']
val_acc = hist_VGG19.history['val_accuracy']

loss = hist_VGG19.history['loss']
val_loss = hist_VGG19.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""##**FINE TUNING VGG19**"""

base_model_VGG19_V15.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model_VGG19_V15.layers))

MODEL_VGG19_V15.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-5),
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

MODEL_VGG19_V15.summary()

start_time = time.time()

fine_tune_epochs = 10
initial_epochs = 10
total_epochs =  initial_epochs + fine_tune_epochs

history_vgg19_fine = MODEL_VGG19_V15.fit(train_generatorDA,
                         validation_data=validation_generatorDA,
                         epochs=total_epochs,
                         steps_per_epoch = len(train_generatorDA),
                         initial_epoch=hist_VGG19.epoch[-1],
                         validation_steps = len(validation_generatorDA)
                         )
print("Running time : --- %s secods ---" % (time.time() - start_time))

acc += history_vgg19_fine.history['accuracy']
val_acc += history_vgg19_fine.history['val_accuracy']

loss += history_vgg19_fine.history['loss']
val_loss += history_vgg19_fine.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

acc_loss_vgg19_finetuneDA = MODEL_VGG19_V15.evaluate(validation_generatorDA)
print('accuracy = ', acc_loss_vgg19_finetuneDA[1]*100)
print('loss     = ', acc_loss_vgg19_finetuneDA[0])

"""##**PERBANDINGAN FASE 1 DENGAN FASE 2**"""

acc_loss_vgg19

print('Akurasi Pada Fase 1 : ', acc_loss_vgg19[1]*100)
print('Loss Pada Fase 1    : ', acc_loss_vgg19[0])
print('==================================================')
print('Akurasi Pada Fase 2 : ', acc_loss_vgg19_finetuneDA[1]*100)
print('Loss Pada Fase 2    : ', acc_loss_vgg19_finetuneDA[0])
print('==================================================')


X = ['Adaptasi','Setelah Fine Tuning']
YAccuracy = [acc_loss_vgg19[1]*100,acc_loss_vgg19_finetuneDA[1]*100,]
ZLoss = [acc_loss_vgg19[0]*100,acc_loss_vgg19_finetuneDA[0]*100,]
  
X_axis = np.arange(len(X))
  
plt.bar(X_axis - 0.1, YAccuracy, 0.2, label = 'Accuracy', )
plt.bar(X_axis + 0.1, ZLoss, 0.2, label = 'Loss', )
  
plt.xticks(X_axis, X)
plt.ylabel("")
plt.ylabel("")
plt.title("Perbandingan Akurasi dan Loss Fase 1 dengan Fase 2")
plt.legend(loc='upper right')
plt.show()

"""##**UJI DENGAN DATA TEST**"""

test_dataset = tf.keras.utils.image_dataset_from_directory(test_path,
                                                                 shuffle=True,
                                                                 batch_size=32,
                                                                 image_size=(200,200))

test_datasetDA = test_datagenDA.flow_from_directory(test_path,
                                                    classes = ['Healthy', 'Early_Blight', 'Late_Blight'],
                                                    class_mode = 'categorical',
                                                    color_mode = "rgb",
                                                    batch_size = 32,
                                                    target_size = (200, 200),
                                                    shuffle = False)

test_class_names = test_dataset.class_names
test_class_names

import seaborn as sns
from sklearn.metrics import precision_score, recall_score,\
                            confusion_matrix, classification_report, \
                            accuracy_score, f1_score, plot_confusion_matrix

pred_VGG19 = np.argmax(MODEL_VGG19_V15.predict(test_datasetDA), axis = -1)

def VGG19_report(predictions):
    ax = plt.subplot()
    sns.heatmap(confusion_matrix(test_datasetDA.classes,pred_VGG19), annot = True, cmap="Greens", fmt='g', ax=ax)
    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
    ax.set_title('Confusion Matrix VGG19');
    ax.xaxis.set_ticklabels(['Healthy', 'Early Blight', 'Late Blight']); ax.yaxis.set_ticklabels(['Healthy', 'Early Blight', 'Late Blight']);

cm3_DA = confusion_matrix(test_datasetDA.classes, pred_VGG19)
VGG19_report(pred_VGG19)
benar_V3 = cm3_DA[0][0] + cm3_DA[1][1] + cm3_DA[2][2]
salah_V3 = cm3_DA[0][1] + cm3_DA[0][2] + cm3_DA[1][0] + cm3_DA[1][2] + cm3_DA[2][0] + cm3_DA[2][1] 
print('Jumlah prediksi benar dari 306 gambar = ', benar_V3)
print('Jumlah prediksi salah dari 306 gambar = ', salah_V3,'\n')

cm3_DA = confusion_matrix(test_datasetDA.classes, pred_VGG19)

print('===================CONFUSION MATRIX VGG19===================== \n')
print('[TP] True positive   = ', cm3_DA[0][0])
print('[FP] False positive  = ', cm3_DA[1][0] + cm3_DA[2][0])
print('[FN] False negative  = ', cm3_DA[0][1] + cm3_DA[0][2])
print('[TN] True negative   = ', cm3_DA[1][1] + cm3_DA[1][2] + cm3_DA[2][1] + cm3_DA[2][2])

TP = cm3_DA[0][0]
FP = cm3_DA[1][0] + cm3_DA[2][0]
FN = cm3_DA[0][1] + cm3_DA[0][2]
TN = cm3_DA[1][1] + cm3_DA[1][2] + cm3_DA[2][1] + cm3_DA[2][2]


Sensitivity_VGG19_DA = TP / ( TP + FN )
Specificity_VGG19_DA = TN / ( TN + FP )

print("\nSensitivity INCV3    = ", Sensitivity_VGG19_DA*100)
print("Spesificity INCV3    = ", Specificity_VGG19_DA*100)

for images_batch, labels_batch in test_dataset.take(1):
  first_image = images_batch[0].numpy().astype('uint8')
  first_label = labels_batch[0]
  plt.imshow(first_image)
  plt.axis('off')
  batch_prediction = MODEL_VGG19_V15.predict(images_batch)
  print('Label Images      : ', test_class_names[first_label])
  print('Prediction Result : ', test_class_names[np.argmax(batch_prediction[0])],'\n')

"""##**PERBANDINGAN KE 3 MODEL**"""

model_inceptionV3 = acc_loss_incV3_DA
print('Akurasi InceptionV3  = ',model_inceptionV3[1]*100)
print('Loss Inception V3    = ',model_inceptionV3[0],'\n')

model_resnet50 = acc_loss_rn50
print('Akurasi ResNet50     = ',model_resnet50[1]*100)
print('Loss ResNet50        = ',model_resnet50[0],'\n')

model_vgg19 = acc_loss_vgg19
print('Akurasi VGG19        = ',model_vgg19[1]*100)
print('Loss VGG19           = ',model_vgg19[0],'\n')

print('================ SETELAH FINE-TUNING ==================''\n')

model_inceptionV3_ft = acc_loss_finetuneDA
print('Akurasi InceptionV3  = ', model_inceptionV3_ft[1]*100)
print('Loss InceptionV3     = ', model_inceptionV3_ft[0],'\n')

model_resnet50_ft = acc_loss_rn50_finetuneDA
print('Akurasi ResNet50     = ', model_resnet50_ft[1]*100)
print('Loss ResNet50        = ', model_resnet50_ft[0],'\n')

model_vgg19_ft = acc_loss_vgg19_finetuneDA
print('Akurasi VGG19        = ', model_vgg19_ft[1]*100)
print('Loss VGG19           = ', model_vgg19_ft[0],'\n')

"""##**SAVE MODEL**"""

MODEL_INCEPTIONV3_V15.save('3CLASS_InceptionV3.hdf5')
MODEL_RESNET50_V15.save('3CLASS_ResNet50.hdf5')
MODEL_VGG19_V15.save('3CLASS_VGG19.hdf5')